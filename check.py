import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "rtzr/ko-gemma-2-9b-it"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# ëª¨ë¸ ë¡œë“œ (ìë™ ì„¤ì •)
model = AutoModelForCausalLM.from_pretrained(model_name)

# í™•ì¸ 1ï¸âƒ£: Configì—ì„œ ì§ì ‘ ê°€ì ¸ì˜¤ê¸°
attn_implementation = model.config.__dict__.get("attn_implementation", "ìë™ ì„¤ì •ë¨ (_attn_implementation_autoset=True)")
print(f"í˜„ì¬ Attention ì„¤ì •: {attn_implementation}")

# í™•ì¸ 2ï¸âƒ£: ëª¨ë¸ì„ ì‹¤ì œë¡œ ì‹¤í–‰í•˜ì—¬ Debugging
input_text = "ì´ ëª¨ë¸ì˜ attention êµ¬í˜„ ë°©ì‹ì€ ë¬´ì—‡ì¸ê°€ìš”?"
input_ids = tokenizer(input_text, return_tensors="pt").input_ids

with torch.no_grad():
    output = model(input_ids)

# í™•ì¸ 3ï¸âƒ£: SDPA ë˜ëŠ” EAGER ì‚¬ìš© ì—¬ë¶€ í™•ì¸
if hasattr(model, "attn_implementation"):
    print(f"ğŸ”¹ ëª¨ë¸ì´ ì‹¤ì œ ì‚¬ìš©í•˜ëŠ” Attention êµ¬í˜„ ë°©ì‹: {model.attn_implementation}")
else:
    print("âš ï¸ ëª¨ë¸ì´ ìë™ ì„¤ì •ëœ Attentionì„ ì‚¬ìš© ì¤‘ (SDPAì¼ ê°€ëŠ¥ì„± ë†’ìŒ)")
